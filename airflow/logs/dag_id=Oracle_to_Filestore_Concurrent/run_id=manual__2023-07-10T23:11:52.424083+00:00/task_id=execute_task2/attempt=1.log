[2023-07-10T23:12:02.225+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:11:52.424083+00:00 [queued]>
[2023-07-10T23:12:02.229+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:11:52.424083+00:00 [queued]>
[2023-07-10T23:12:02.229+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-10T23:12:02.236+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonDecoratedOperator): execute_task2> on 2023-07-10 23:11:52.424083+00:00
[2023-07-10T23:12:02.240+0000] {standard_task_runner.py:57} INFO - Started process 361 to run task
[2023-07-10T23:12:02.240+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:12:02.242+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'Oracle_to_Filestore_Concurrent', 'execute_task2', 'manual__2023-07-10T23:11:52.424083+00:00', '--job-id', '963', '--raw', '--subdir', 'DAGS_FOLDER/Python-Delta-Lake/Dags/SparkOracle2FileStoreDag.py', '--cfg-path', '/tmp/tmpp1byu7ll']
[2023-07-10T23:12:02.244+0000] {standard_task_runner.py:85} INFO - Job 963: Subtask execute_task2
[2023-07-10T23:12:02.268+0000] {task_command.py:410} INFO - Running <TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:11:52.424083+00:00 [running]> on host 876817aabcc6
[2023-07-10T23:12:02.307+0000] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='phomsophaN@saccounty.net' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Oracle_to_Filestore_Concurrent' AIRFLOW_CTX_TASK_ID='execute_task2' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T23:11:52.424083+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-07-10T23:11:52.424083+00:00'
[2023-07-10T23:12:02.307+0000] {logging_mixin.py:149} INFO - Spark Connection check - session found
[2023-07-10T23:12:02.313+0000] {logging_mixin.py:149} INFO - Spark Connection check - session found
[2023-07-10T23:12:02.316+0000] {logging_mixin.py:149} INFO - Attempt to read Msql Table by query: select * from DeltaLake.OracleTablesLog
        where OracleRows < 1000000 AND OracleRows > 10000 AND OracleSizeInMB < 2500 AND CountyFilter IS NOT NULL and PrimaryKeyColumn !=''
[2023-07-10T23:12:02.807+0000] {clientserver.py:505} INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer
[2023-07-10T23:12:02.807+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:12:02.808+0000] {java_gateway.py:1052} INFO - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
[2023-07-10T23:12:02.814+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:12:03.094+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.095+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.096+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.099+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.100+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.100+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.101+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.102+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:12:03.108+0000] {SparkOracle2FileStore.py:81} INFO - Skipping log tables and reading tables directly
[2023-07-10T23:12:03.109+0000] {SparkOracle2FileStore.py:91} INFO - processing 1 out of 11
[2023-07-10T23:12:03.109+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.OCAT
[2023-07-10T23:12:03.109+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:04.768+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.OCAT
[2023-07-10T23:12:04.768+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:06.141+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:12:06.708+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:12:06.709+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:08.955+0000] {SparkOracle2FileStore.py:114} INFO - 176399
[2023-07-10T23:12:08.955+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:10.152+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/OCAT
[2023-07-10T23:12:10.152+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:12:10.152+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:12:32.496+0000] {local_task_job_runner.py:291} WARNING - State of this instance has been externally set to success. Terminating instance.
[2023-07-10T23:12:32.497+0000] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 361. PIDs of all processes in the group: [361]
[2023-07-10T23:12:32.497+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 361
[2023-07-10T23:12:32.497+0000] {taskinstance.py:1517} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-07-10T23:12:32.498+0000] {clientserver.py:538} INFO - Error while receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1519, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2023-07-10T23:12:32.498+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:12:32.498+0000] {java_gateway.py:1055} ERROR - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/usr/local/lib/python3.10/socket.py", line 705, in readinto
    return self._sock.recv_into(b)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 1519, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
[2023-07-10T23:12:32.499+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:12:32.499+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o144.save
[2023-07-10T23:12:32.501+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.OCAT
[2023-07-10T23:12:32.599+0000] {SparkOracle2FileStore.py:91} INFO - processing 2 out of 11
[2023-07-10T23:12:32.599+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:12:32.599+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:34.062+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:12:34.062+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:35.215+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:12:35.851+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:12:35.851+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:37.068+0000] {SparkOracle2FileStore.py:114} INFO - 17802
[2023-07-10T23:12:37.068+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:38.283+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/WDTIP_EXCEED_CLOCK
[2023-07-10T23:12:38.283+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:12:38.283+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:12:41.879+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:12:41.951+0000] {SparkOracle2FileStore.py:91} INFO - processing 3 out of 11
[2023-07-10T23:12:41.951+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.AUTO_ACTN
[2023-07-10T23:12:41.951+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:43.109+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.AUTO_ACTN
[2023-07-10T23:12:43.109+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:44.528+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:12:45.037+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:12:45.037+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:46.227+0000] {SparkOracle2FileStore.py:114} INFO - 13113
[2023-07-10T23:12:46.227+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:47.398+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/AUTO_ACTN
[2023-07-10T23:12:47.398+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:12:47.398+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:12:49.273+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.AUTO_ACTN
[2023-07-10T23:12:49.347+0000] {SparkOracle2FileStore.py:91} INFO - processing 4 out of 11
[2023-07-10T23:12:49.347+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:12:49.347+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:50.433+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:12:50.433+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:12:51.609+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:12:52.176+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:12:52.177+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:53.308+0000] {SparkOracle2FileStore.py:114} INFO - 21414
[2023-07-10T23:12:53.308+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:12:54.440+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/EBT_FRAUD_ACTIV
[2023-07-10T23:12:54.440+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:12:54.440+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:12:58.306+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:12:58.372+0000] {SparkOracle2FileStore.py:190} INFO - Tables Thresholds reached, restarting spark context
[2023-07-10T23:12:58.372+0000] {logging_mixin.py:149} INFO - Stopping Spark Context
[2023-07-10T23:13:32.502+0000] {process_utils.py:149} WARNING - process psutil.Process(pid=361, name='airflow task ru', status='sleeping', started='23:12:02') did not respond to SIGTERM. Trying SIGKILL
[2023-07-10T23:13:32.503+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGKILL to group 361
[2023-07-10T23:13:32.510+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=361, name='airflow task ru', status='terminated', exitcode=<Negsignal.SIGKILL: -9>, started='23:12:02') (361) terminated with exit code Negsignal.SIGKILL
[2023-07-10T23:13:32.510+0000] {standard_task_runner.py:172} ERROR - Job 963 was killed before it finished (likely due to running out of memory)
