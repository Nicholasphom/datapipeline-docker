[2023-07-10T23:26:40.192+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:26:32.127487+00:00 [queued]>
[2023-07-10T23:26:40.197+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:26:32.127487+00:00 [queued]>
[2023-07-10T23:26:40.197+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-07-10T23:26:40.204+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonDecoratedOperator): execute_task2> on 2023-07-10 23:26:32.127487+00:00
[2023-07-10T23:26:40.208+0000] {standard_task_runner.py:57} INFO - Started process 1225 to run task
[2023-07-10T23:26:40.208+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:26:40.211+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'Oracle_to_Filestore_Concurrent', 'execute_task2', 'manual__2023-07-10T23:26:32.127487+00:00', '--job-id', '964', '--raw', '--subdir', 'DAGS_FOLDER/Python-Delta-Lake/Dags/SparkOracle2FileStoreDag.py', '--cfg-path', '/tmp/tmp8_qnks9q']
[2023-07-10T23:26:40.212+0000] {standard_task_runner.py:85} INFO - Job 964: Subtask execute_task2
[2023-07-10T23:26:40.239+0000] {task_command.py:410} INFO - Running <TaskInstance: Oracle_to_Filestore_Concurrent.execute_task2 manual__2023-07-10T23:26:32.127487+00:00 [running]> on host 876817aabcc6
[2023-07-10T23:26:40.280+0000] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='phomsophaN@saccounty.net' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Oracle_to_Filestore_Concurrent' AIRFLOW_CTX_TASK_ID='execute_task2' AIRFLOW_CTX_EXECUTION_DATE='2023-07-10T23:26:32.127487+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-07-10T23:26:32.127487+00:00'
[2023-07-10T23:26:40.280+0000] {logging_mixin.py:149} INFO - Spark Connection check - session found
[2023-07-10T23:26:40.286+0000] {logging_mixin.py:149} INFO - Spark Connection check - session found
[2023-07-10T23:26:40.290+0000] {logging_mixin.py:149} INFO - Attempt to read Msql Table by query: select * from DeltaLake.OracleTablesLog
        where OracleRows < 1000000 AND OracleRows > 10000 AND OracleSizeInMB < 2500 AND CountyFilter IS NOT NULL and PrimaryKeyColumn !=''
[2023-07-10T23:26:40.829+0000] {clientserver.py:505} INFO - Error while sending or receiving.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer
[2023-07-10T23:26:40.830+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:26:40.830+0000] {java_gateway.py:1052} INFO - Exception while sending command.
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 503, in send_command
    self.socket.sendall(command.encode("utf-8"))
ConnectionResetError: [Errno 104] Connection reset by peer

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
  File "/home/airflow/.local/lib/python3.10/site-packages/py4j/clientserver.py", line 506, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending
[2023-07-10T23:26:40.832+0000] {clientserver.py:543} INFO - Closing down clientserver connection
[2023-07-10T23:26:41.473+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.475+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.475+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.479+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.480+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.480+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.483+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.484+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-07-10T23:26:41.491+0000] {SparkOracle2FileStore.py:81} INFO - Skipping log tables and reading tables directly
[2023-07-10T23:26:41.491+0000] {SparkOracle2FileStore.py:91} INFO - processing 1 out of 11
[2023-07-10T23:26:41.492+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.OCAT
[2023-07-10T23:26:41.492+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:26:43.137+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.OCAT
[2023-07-10T23:26:43.137+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:26:44.322+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:26:44.840+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:26:44.840+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:26:46.213+0000] {SparkOracle2FileStore.py:114} INFO - 176399
[2023-07-10T23:26:46.214+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:26:47.486+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/OCAT
[2023-07-10T23:26:47.486+0000] {logging_mixin.py:149} INFO - Returning True
[2023-07-10T23:26:47.486+0000] {SparkOracle2FileStore.py:128} INFO - Should go in here if table dir exists
[2023-07-10T23:26:48.625+0000] {SparkOracle2FileStore.py:141} INFO - read dir table should not fail and go in here...
[2023-07-10T23:27:13.538+0000] {SparkOracle2FileStore.py:154} INFO - no rows to upsert on table OCAT
[2023-07-10T23:27:33.673+0000] {SparkOracle2FileStore.py:158} INFO - Updating rows on table OCAT
[2023-07-10T23:28:21.504+0000] {SparkOracle2FileStore.py:172} INFO - no rows to delete on table OCAT
[2023-07-10T23:28:21.507+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.OCAT
[2023-07-10T23:28:21.616+0000] {SparkOracle2FileStore.py:91} INFO - processing 2 out of 11
[2023-07-10T23:28:21.616+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:21.616+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:22.744+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:22.744+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:23.899+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:28:24.500+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:28:24.500+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:25.764+0000] {SparkOracle2FileStore.py:114} INFO - 17802
[2023-07-10T23:28:25.764+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:26.941+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:26.941+0000] {logging_mixin.py:149} INFO - Returning True
[2023-07-10T23:28:26.941+0000] {SparkOracle2FileStore.py:128} INFO - Should go in here if table dir exists
[2023-07-10T23:28:26.998+0000] {SparkOracle2FileStore.py:141} INFO - read dir table should not fail and go in here...
[2023-07-10T23:28:29.457+0000] {SparkOracle2FileStore.py:154} INFO - no rows to upsert on table WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:31.014+0000] {SparkOracle2FileStore.py:158} INFO - Updating rows on table WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:36.085+0000] {SparkOracle2FileStore.py:172} INFO - no rows to delete on table WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:36.086+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.WDTIP_EXCEED_CLOCK
[2023-07-10T23:28:36.159+0000] {SparkOracle2FileStore.py:91} INFO - processing 3 out of 11
[2023-07-10T23:28:36.159+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.AUTO_ACTN
[2023-07-10T23:28:36.159+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:37.294+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.AUTO_ACTN
[2023-07-10T23:28:37.294+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:38.365+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:28:38.916+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:28:38.917+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:40.076+0000] {SparkOracle2FileStore.py:114} INFO - 13113
[2023-07-10T23:28:40.076+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:41.190+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/AUTO_ACTN
[2023-07-10T23:28:41.190+0000] {logging_mixin.py:149} INFO - Returning True
[2023-07-10T23:28:41.191+0000] {SparkOracle2FileStore.py:128} INFO - Should go in here if table dir exists
[2023-07-10T23:28:41.236+0000] {SparkOracle2FileStore.py:141} INFO - read dir table should not fail and go in here...
[2023-07-10T23:28:43.171+0000] {SparkOracle2FileStore.py:154} INFO - no rows to upsert on table AUTO_ACTN
[2023-07-10T23:28:44.940+0000] {SparkOracle2FileStore.py:158} INFO - Updating rows on table AUTO_ACTN
[2023-07-10T23:28:49.787+0000] {SparkOracle2FileStore.py:172} INFO - no rows to delete on table AUTO_ACTN
[2023-07-10T23:28:49.787+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.AUTO_ACTN
[2023-07-10T23:28:49.858+0000] {SparkOracle2FileStore.py:91} INFO - processing 4 out of 11
[2023-07-10T23:28:49.858+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:28:49.859+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:51.413+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:28:51.413+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:28:53.648+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:28:55.050+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:28:55.050+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:57.235+0000] {SparkOracle2FileStore.py:114} INFO - 21414
[2023-07-10T23:28:57.235+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:28:59.426+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/EBT_FRAUD_ACTIV
[2023-07-10T23:28:59.426+0000] {logging_mixin.py:149} INFO - Returning True
[2023-07-10T23:28:59.426+0000] {SparkOracle2FileStore.py:128} INFO - Should go in here if table dir exists
[2023-07-10T23:28:59.470+0000] {SparkOracle2FileStore.py:141} INFO - read dir table should not fail and go in here...
[2023-07-10T23:29:02.899+0000] {SparkOracle2FileStore.py:154} INFO - no rows to upsert on table EBT_FRAUD_ACTIV
[2023-07-10T23:29:05.761+0000] {SparkOracle2FileStore.py:158} INFO - Updating rows on table EBT_FRAUD_ACTIV
[2023-07-10T23:29:16.457+0000] {SparkOracle2FileStore.py:172} INFO - no rows to delete on table EBT_FRAUD_ACTIV
[2023-07-10T23:29:16.458+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.EBT_FRAUD_ACTIV
[2023-07-10T23:29:16.526+0000] {SparkOracle2FileStore.py:190} INFO - Tables Thresholds reached, restarting spark context
[2023-07-10T23:29:16.526+0000] {logging_mixin.py:149} INFO - Stopping Spark Context
[2023-07-10T23:56:42.464+0000] {local_task_job_runner.py:291} WARNING - State of this instance has been externally set to failed. Terminating instance.
[2023-07-10T23:56:42.465+0000] {process_utils.py:131} INFO - Sending Signals.SIGTERM to group 1225. PIDs of all processes in the group: [1225]
[2023-07-10T23:56:42.465+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGTERM to group 1225
[2023-07-10T23:56:42.465+0000] {taskinstance.py:1517} ERROR - Received SIGTERM. Terminating subprocesses.
[2023-07-10T23:56:42.465+0000] {SparkOracle2FileStore.py:194} INFO - Error Processing Table PR_LRS.EBT_FRAUD_ACTIV:Task received SIGTERM signal
[2023-07-10T23:56:42.465+0000] {SparkOracle2FileStore.py:91} INFO - processing 5 out of 11
[2023-07-10T23:56:42.465+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.GRP_HOMES
[2023-07-10T23:56:42.465+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:43.042+0000] {logging_mixin.py:149} INFO - Could not get min count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:43.042+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.GRP_HOMES
[2023-07-10T23:56:43.042+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:43.589+0000] {logging_mixin.py:149} INFO - Could not get max count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:43.589+0000] {SparkOracle2FileStore.py:101} INFO - could not reliablily get upper and lower bounds of table PR_LRS.GRP_HOMES, reading without partition 
[2023-07-10T23:56:43.589+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:56:44.148+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:56:44.148+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:44.782+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:44.782+0000] {SparkOracle2FileStore.py:114} INFO - None
[2023-07-10T23:56:44.782+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:45.359+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:45.359+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/GRP_HOMES
[2023-07-10T23:56:45.359+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:56:45.360+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:56:45.477+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o392.save.
: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:242)
	at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:368)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:331)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:97)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:171)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-10T23:56:45.478+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.GRP_HOMES
[2023-07-10T23:56:45.541+0000] {SparkOracle2FileStore.py:91} INFO - processing 6 out of 11
[2023-07-10T23:56:45.541+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.FUND_CODE_MAP
[2023-07-10T23:56:45.541+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:46.044+0000] {logging_mixin.py:149} INFO - Could not get min count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:46.045+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.FUND_CODE_MAP
[2023-07-10T23:56:46.045+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:46.550+0000] {logging_mixin.py:149} INFO - Could not get max count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:46.551+0000] {SparkOracle2FileStore.py:101} INFO - could not reliablily get upper and lower bounds of table PR_LRS.FUND_CODE_MAP, reading without partition 
[2023-07-10T23:56:46.551+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:56:47.138+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:56:47.138+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:47.764+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:47.764+0000] {SparkOracle2FileStore.py:114} INFO - None
[2023-07-10T23:56:47.764+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:48.330+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:48.330+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/FUND_CODE_MAP
[2023-07-10T23:56:48.331+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:56:48.331+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:56:48.387+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o437.save.
: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:242)
	at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:368)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:331)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:97)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:171)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-10T23:56:48.387+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.FUND_CODE_MAP
[2023-07-10T23:56:48.451+0000] {SparkOracle2FileStore.py:91} INFO - processing 7 out of 11
[2023-07-10T23:56:48.451+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-07-10T23:56:48.451+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:48.954+0000] {logging_mixin.py:149} INFO - Could not get min count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:48.954+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-07-10T23:56:48.954+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:49.493+0000] {logging_mixin.py:149} INFO - Could not get max count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:49.493+0000] {SparkOracle2FileStore.py:101} INFO - could not reliablily get upper and lower bounds of table PR_LRS.C4Y_OTHER_PGM_ASSIST, reading without partition 
[2023-07-10T23:56:49.493+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:56:50.014+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:56:50.014+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:50.576+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:50.576+0000] {SparkOracle2FileStore.py:114} INFO - None
[2023-07-10T23:56:50.576+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:51.183+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:51.183+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/C4Y_OTHER_PGM_ASSIST
[2023-07-10T23:56:51.183+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:56:51.183+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:56:51.223+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o482.save.
: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:242)
	at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:368)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:331)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:97)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:171)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-10T23:56:51.223+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-07-10T23:56:51.297+0000] {SparkOracle2FileStore.py:91} INFO - processing 8 out of 11
[2023-07-10T23:56:51.297+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.COLLAB_CONTRACT
[2023-07-10T23:56:51.297+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:51.879+0000] {logging_mixin.py:149} INFO - Could not get min count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:51.879+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.COLLAB_CONTRACT
[2023-07-10T23:56:51.879+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:52.423+0000] {logging_mixin.py:149} INFO - Could not get max count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:52.423+0000] {SparkOracle2FileStore.py:101} INFO - could not reliablily get upper and lower bounds of table PR_LRS.COLLAB_CONTRACT, reading without partition 
[2023-07-10T23:56:52.423+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:56:52.993+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:56:52.993+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:53.618+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:53.619+0000] {SparkOracle2FileStore.py:114} INFO - None
[2023-07-10T23:56:53.619+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:54.130+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:54.130+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/COLLAB_CONTRACT
[2023-07-10T23:56:54.130+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:56:54.130+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:56:54.164+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o527.save.
: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:242)
	at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:368)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:331)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:97)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:171)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-10T23:56:54.165+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.COLLAB_CONTRACT
[2023-07-10T23:56:54.229+0000] {SparkOracle2FileStore.py:91} INFO - processing 9 out of 11
[2023-07-10T23:56:54.230+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_APPRCH_CLOCK
[2023-07-10T23:56:54.230+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:54.969+0000] {logging_mixin.py:149} INFO - Could not get min count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:54.969+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_APPRCH_CLOCK
[2023-07-10T23:56:54.969+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-07-10T23:56:55.520+0000] {logging_mixin.py:149} INFO - Could not get max count: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:55.520+0000] {SparkOracle2FileStore.py:101} INFO - could not reliablily get upper and lower bounds of table PR_LRS.WDTIP_APPRCH_CLOCK, reading without partition 
[2023-07-10T23:56:55.520+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-07-10T23:56:56.048+0000] {SparkOracle2FileStore.py:111} INFO - Filter Processor Disabled
[2023-07-10T23:56:56.048+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:56.571+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:56.571+0000] {SparkOracle2FileStore.py:114} INFO - None
[2023-07-10T23:56:56.571+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-07-10T23:56:57.094+0000] {logging_mixin.py:149} INFO - Error in reading oracle table by rowcount: 'NoneType' object has no attribute 'setCallSite'
[2023-07-10T23:56:57.094+0000] {logging_mixin.py:149} INFO - Checking if directory exists:/home/nicholas/dev/output/WDTIP_APPRCH_CLOCK
[2023-07-10T23:56:57.094+0000] {logging_mixin.py:149} INFO - Returning False
[2023-07-10T23:56:57.094+0000] {SparkOracle2FileStore.py:176} INFO - Should not go in here if table dir exists
[2023-07-10T23:56:57.126+0000] {logging_mixin.py:149} INFO - Error in writing dataframe:An error occurred while calling o572.save.
: java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:529)
	at scala.None$.get(Option.scala:527)
	at org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$.metrics(BasicWriteStatsTracker.scala:242)
	at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:368)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:353)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:328)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:212)
	at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:209)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:331)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:97)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:92)
	at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:171)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:303)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
[2023-07-10T23:56:57.127+0000] {SparkOracle2FileStore.py:181} INFO - Table Processing complete for PR_LRS.WDTIP_APPRCH_CLOCK
[2023-07-10T23:56:57.188+0000] {SparkOracle2FileStore.py:190} INFO - Tables Thresholds reached, restarting spark context
[2023-07-10T23:56:57.188+0000] {logging_mixin.py:149} INFO - Stopping Spark Context
[2023-07-10T23:57:42.467+0000] {process_utils.py:149} WARNING - process psutil.Process(pid=1225, name='airflow task ru', status='sleeping', started='23:26:40') did not respond to SIGTERM. Trying SIGKILL
[2023-07-10T23:57:42.467+0000] {process_utils.py:86} INFO - Sending the signal Signals.SIGKILL to group 1225
[2023-07-10T23:57:42.474+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=1225, name='airflow task ru', status='terminated', exitcode=<Negsignal.SIGKILL: -9>, started='23:26:40') (1225) terminated with exit code Negsignal.SIGKILL
[2023-07-10T23:57:42.474+0000] {standard_task_runner.py:172} ERROR - Job 964 was killed before it finished (likely due to running out of memory)
