[2023-06-30T07:32:20.666+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Oracle_to_Filestore.execute_function_task manual__2023-06-30T07:32:19.736968+00:00 [queued]>
[2023-06-30T07:32:20.671+0000] {taskinstance.py:1103} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Oracle_to_Filestore.execute_function_task manual__2023-06-30T07:32:19.736968+00:00 [queued]>
[2023-06-30T07:32:20.671+0000] {taskinstance.py:1308} INFO - Starting attempt 1 of 1
[2023-06-30T07:32:20.678+0000] {taskinstance.py:1327} INFO - Executing <Task(_PythonDecoratedOperator): execute_function_task> on 2023-06-30 07:32:19.736968+00:00
[2023-06-30T07:32:20.682+0000] {standard_task_runner.py:57} INFO - Started process 54 to run task
[2023-06-30T07:32:20.684+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'Oracle_to_Filestore', 'execute_function_task', 'manual__2023-06-30T07:32:19.736968+00:00', '--job-id', '674', '--raw', '--subdir', 'DAGS_FOLDER/Python-Delta-Lake/Dags/SparkOracle2FileStoreDag.py', '--cfg-path', '/tmp/tmpynpkyuv1']
[2023-06-30T07:32:20.685+0000] {standard_task_runner.py:85} INFO - Job 674: Subtask execute_function_task
[2023-06-30T07:32:20.710+0000] {task_command.py:410} INFO - Running <TaskInstance: Oracle_to_Filestore.execute_function_task manual__2023-06-30T07:32:19.736968+00:00 [running]> on host 4e0859c08c65
[2023-06-30T07:32:20.754+0000] {taskinstance.py:1545} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='phomsophaN@saccounty.net' AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='Oracle_to_Filestore' AIRFLOW_CTX_TASK_ID='execute_function_task' AIRFLOW_CTX_EXECUTION_DATE='2023-06-30T07:32:19.736968+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2023-06-30T07:32:19.736968+00:00'
[2023-06-30T07:32:20.754+0000] {logging_mixin.py:149} INFO - SparkSession Not found, creating one
[2023-06-30T07:32:20.755+0000] {logging_mixin.py:149} INFO - setting up configured spark connection Cluster
[2023-06-30T07:32:20.755+0000] {logging_mixin.py:149} INFO - Initializing support for delta lake spark context
[2023-06-30T07:32:23.526+0000] {logging_mixin.py:149} INFO - Returning Spark Context
[2023-06-30T07:32:23.526+0000] {client.py:192} INFO - Instantiated <InsecureClient(url='http://namenode:50070')>.
[2023-06-30T07:32:23.527+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:32:23.538+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:32:23.539+0000] {logging_mixin.py:149} INFO - HDFS Detected and Connected
[2023-06-30T07:32:23.539+0000] {logging_mixin.py:149} INFO - Spark Connection check - session found
[2023-06-30T07:32:24.050+0000] {logging_mixin.py:149} INFO - Attempt to read Msql Table by query: select * from DeltaLake.OracleTablesLog
        where OracleRows < 1000000 AND OracleRows > 10000 AND OracleSizeInMB < 2500 AND CountyFilter IS NOT NULL and PrimaryKeyColumn !=''
[2023-06-30T07:32:27.498+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.499+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.505+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.505+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.506+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.507+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.508+0000] {warnings.py:109} WARNING - /home/***/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:248: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead
  series = series.astype(t, copy=False)

[2023-06-30T07:32:27.518+0000] {SparkOracle2FileStore.py:62} INFO - ['PR_LRS.FOSTER_FAM_AGNC', 'PR_LRS.EBT_EXPG_EXCPT', 'PR_LRS.ENCL_ESTIMATE', 'PR_LRS.UNIT', 'PR_LRS.IEVS_SAVE_EXCEPT', 'PR_LRS.WTW_24_MONTH_SIGN_DATE', 'PR_LRS.RULES_ADMIN', 'PR_LRS.COUNTY_PARAMTR_ADMIN_HST', 'PR_LRS.C4Y_PREG', 'PR_LRS.IEVS_PRISON_MATCH', 'PR_LRS.ORG_ACCT', 'PR_LRS.TRAIN_PGM', 'PR_LRS.IFD_SSI', 'PR_LRS.C4Y_PERS_NON_COMPL', 'PR_LRS.POS', 'PR_LRS.ADH_SUMM', 'PR_LRS.WDTIP_PGM_PARTICPTN', 'PR_LRS.HEAR_SUMM', 'PR_LRS.WDTIP_ALERT', 'PR_LRS.IHSS_REFRL', 'PR_LRS.SFIS_COUNTY_INFO', 'PR_LRS.STAFF', 'PR_LRS.MEDS_ALERT_CONFIG', 'PR_LRS.BATCH_JOB', 'PR_LRS.IHSS_CASE', 'PR_LRS.RULES_ADMIN_HST', 'PR_LRS.TAX_INTRCPT', 'PR_LRS.WTW_24_MONTH_SIGN_DATE_HST', 'PR_LRS.COUNTY_PARAMTR_ADMIN', 'PR_LRS.OCAT', 'PR_LRS.WDTIP_EXCEED_CLOCK', 'PR_LRS.AUTO_ACTN', 'PR_LRS.IFD_DUPL_DETL', 'PR_LRS.EBT_FRAUD_ACTIV', 'PR_LRS.GA_GR_TIME_LIMIT', 'PR_LRS.GRP_HOMES', 'PR_LRS.ASSET_VERIF', 'PR_LRS.FUND_CODE_MAP', 'PR_LRS.C4Y_OTHER_PGM_ASSIST', 'PR_LRS.IEVS_FF_MEDS', 'PR_LRS.COLLAB_CONTRACT', 'PR_LRS.WDTIP_APPRCH_CLOCK', 'PR_LRS.EXTRNL_STAFF_COUNTY_STAT', 'PR_LRS.VEND_IDENTIF', 'PR_LRS.OCAT_PERS', 'PR_LRS.MEDS_ALERT_CONFIG_HST', 'PR_LRS.WPR_SAMPLE_CASE', 'PR_LRS.PERS_TIME_TRACK_DETL']
[2023-06-30T07:32:27.519+0000] {SparkOracle2FileStore.py:80} INFO - processing 1 out of 48
[2023-06-30T07:32:27.519+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.FOSTER_FAM_AGNC
[2023-06-30T07:32:27.519+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:32:29.754+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.FOSTER_FAM_AGNC
[2023-06-30T07:32:29.754+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:32:31.117+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:32:31.693+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:32:31.694+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:32:32.853+0000] {SparkOracle2FileStore.py:103} INFO - 34262
[2023-06-30T07:32:32.853+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:32:33.961+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/FOSTER_FAM_AGNC exists...
[2023-06-30T07:32:33.961+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:32:33.966+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:32:33.966+0000] {client.py:320} INFO - Fetching status for '/dev/FOSTER_FAM_AGNC'.
[2023-06-30T07:32:33.969+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:32:33.969+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:32:35.790+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:32:42.946+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table FOSTER_FAM_AGNC
[2023-06-30T07:32:45.433+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table FOSTER_FAM_AGNC
[2023-06-30T07:32:54.886+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table FOSTER_FAM_AGNC
[2023-06-30T07:32:54.886+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.FOSTER_FAM_AGNC
[2023-06-30T07:32:54.890+0000] {SparkOracle2FileStore.py:80} INFO - processing 2 out of 48
[2023-06-30T07:32:54.890+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.EBT_EXPG_EXCPT
[2023-06-30T07:32:54.890+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:32:55.926+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.EBT_EXPG_EXCPT
[2023-06-30T07:32:55.926+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:32:57.042+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:32:57.567+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:32:57.567+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:32:58.756+0000] {SparkOracle2FileStore.py:103} INFO - 316506
[2023-06-30T07:32:58.756+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:32:59.973+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/EBT_EXPG_EXCPT exists...
[2023-06-30T07:32:59.973+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:32:59.978+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:32:59.978+0000] {client.py:320} INFO - Fetching status for '/dev/EBT_EXPG_EXCPT'.
[2023-06-30T07:32:59.981+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:32:59.981+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:33:00.095+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:33:14.994+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table EBT_EXPG_EXCPT
[2023-06-30T07:33:29.666+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table EBT_EXPG_EXCPT
[2023-06-30T07:34:02.984+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table EBT_EXPG_EXCPT
[2023-06-30T07:34:02.985+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.EBT_EXPG_EXCPT
[2023-06-30T07:34:02.985+0000] {SparkOracle2FileStore.py:80} INFO - processing 3 out of 48
[2023-06-30T07:34:02.986+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.ENCL_ESTIMATE
[2023-06-30T07:34:02.986+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:04.058+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.ENCL_ESTIMATE
[2023-06-30T07:34:04.058+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:05.148+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:34:05.673+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:34:05.673+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:06.719+0000] {SparkOracle2FileStore.py:103} INFO - 25979
[2023-06-30T07:34:06.719+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:07.757+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/ENCL_ESTIMATE exists...
[2023-06-30T07:34:07.757+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:34:07.764+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:34:07.764+0000] {client.py:320} INFO - Fetching status for '/dev/ENCL_ESTIMATE'.
[2023-06-30T07:34:07.767+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:34:07.767+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:34:07.831+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:34:09.824+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table ENCL_ESTIMATE
[2023-06-30T07:34:11.285+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table ENCL_ESTIMATE
[2023-06-30T07:34:15.982+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table ENCL_ESTIMATE
[2023-06-30T07:34:15.982+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.ENCL_ESTIMATE
[2023-06-30T07:34:15.983+0000] {SparkOracle2FileStore.py:80} INFO - processing 4 out of 48
[2023-06-30T07:34:15.983+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.UNIT
[2023-06-30T07:34:15.983+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:17.074+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.UNIT
[2023-06-30T07:34:17.074+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:18.098+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:34:18.648+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:34:18.648+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:19.767+0000] {SparkOracle2FileStore.py:103} INFO - 42879
[2023-06-30T07:34:19.767+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:20.881+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/UNIT exists...
[2023-06-30T07:34:20.881+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:34:20.885+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:34:20.885+0000] {client.py:320} INFO - Fetching status for '/dev/UNIT'.
[2023-06-30T07:34:20.887+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:34:20.887+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:34:20.945+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:34:23.406+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table UNIT
[2023-06-30T07:34:25.444+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table UNIT
[2023-06-30T07:34:31.281+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table UNIT
[2023-06-30T07:34:31.281+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.UNIT
[2023-06-30T07:34:31.282+0000] {SparkOracle2FileStore.py:80} INFO - processing 5 out of 48
[2023-06-30T07:34:31.282+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IEVS_SAVE_EXCEPT
[2023-06-30T07:34:31.282+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:32.281+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IEVS_SAVE_EXCEPT
[2023-06-30T07:34:32.281+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:34:33.312+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:34:33.821+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:34:33.821+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:35.130+0000] {SparkOracle2FileStore.py:103} INFO - 397776
[2023-06-30T07:34:35.130+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:34:36.417+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IEVS_SAVE_EXCEPT exists...
[2023-06-30T07:34:36.417+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:34:36.421+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:34:36.421+0000] {client.py:320} INFO - Fetching status for '/dev/IEVS_SAVE_EXCEPT'.
[2023-06-30T07:34:36.424+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:34:36.424+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:34:36.476+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:34:58.463+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IEVS_SAVE_EXCEPT
[2023-06-30T07:35:20.831+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IEVS_SAVE_EXCEPT
[2023-06-30T07:36:11.669+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IEVS_SAVE_EXCEPT
[2023-06-30T07:36:11.669+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IEVS_SAVE_EXCEPT
[2023-06-30T07:36:11.670+0000] {SparkOracle2FileStore.py:80} INFO - processing 6 out of 48
[2023-06-30T07:36:11.670+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:36:11.670+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:36:12.738+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:36:12.738+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:36:13.791+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:36:14.311+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:36:14.311+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:36:15.549+0000] {SparkOracle2FileStore.py:103} INFO - 830945
[2023-06-30T07:36:15.549+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:36:16.884+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WTW_24_MONTH_SIGN_DATE exists...
[2023-06-30T07:36:16.884+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:36:16.888+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:36:16.888+0000] {client.py:320} INFO - Fetching status for '/dev/WTW_24_MONTH_SIGN_DATE'.
[2023-06-30T07:36:16.890+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:36:16.890+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:36:16.940+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:36:44.763+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:37:12.052+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:38:11.876+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:38:11.876+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WTW_24_MONTH_SIGN_DATE
[2023-06-30T07:38:11.877+0000] {SparkOracle2FileStore.py:80} INFO - processing 7 out of 48
[2023-06-30T07:38:11.877+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.RULES_ADMIN
[2023-06-30T07:38:11.877+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:12.959+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.RULES_ADMIN
[2023-06-30T07:38:12.959+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:14.038+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:38:14.590+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:38:14.590+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:15.763+0000] {SparkOracle2FileStore.py:103} INFO - 25885
[2023-06-30T07:38:15.763+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:16.826+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/RULES_ADMIN exists...
[2023-06-30T07:38:16.826+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:38:16.830+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:38:16.830+0000] {client.py:320} INFO - Fetching status for '/dev/RULES_ADMIN'.
[2023-06-30T07:38:16.833+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:38:16.833+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:38:16.895+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:38:18.776+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table RULES_ADMIN
[2023-06-30T07:38:20.034+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table RULES_ADMIN
[2023-06-30T07:38:24.508+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table RULES_ADMIN
[2023-06-30T07:38:24.508+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.RULES_ADMIN
[2023-06-30T07:38:24.509+0000] {SparkOracle2FileStore.py:80} INFO - processing 8 out of 48
[2023-06-30T07:38:24.509+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column HST_ID on table PR_LRS.COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:24.509+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:25.704+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column HST_ID on table PR_LRS.COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:25.704+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:26.815+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:38:27.333+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:38:27.333+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:28.480+0000] {SparkOracle2FileStore.py:103} INFO - 48818
[2023-06-30T07:38:28.480+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:29.643+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/COUNTY_PARAMTR_ADMIN_HST exists...
[2023-06-30T07:38:29.643+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:38:29.647+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:38:29.647+0000] {client.py:320} INFO - Fetching status for '/dev/COUNTY_PARAMTR_ADMIN_HST'.
[2023-06-30T07:38:29.650+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:38:29.650+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:38:29.698+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:38:32.772+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:35.690+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:42.384+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:42.385+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.COUNTY_PARAMTR_ADMIN_HST
[2023-06-30T07:38:42.385+0000] {SparkOracle2FileStore.py:80} INFO - processing 9 out of 48
[2023-06-30T07:38:42.386+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.C4Y_PREG
[2023-06-30T07:38:42.386+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:43.455+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.C4Y_PREG
[2023-06-30T07:38:43.455+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:38:44.550+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:38:45.143+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:38:45.143+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:46.255+0000] {SparkOracle2FileStore.py:103} INFO - 87659
[2023-06-30T07:38:46.255+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:38:47.336+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/C4Y_PREG exists...
[2023-06-30T07:38:47.336+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:38:47.340+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:38:47.340+0000] {client.py:320} INFO - Fetching status for '/dev/C4Y_PREG'.
[2023-06-30T07:38:47.343+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:38:47.343+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:38:47.443+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:38:50.883+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table C4Y_PREG
[2023-06-30T07:38:53.917+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table C4Y_PREG
[2023-06-30T07:39:01.903+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table C4Y_PREG
[2023-06-30T07:39:01.903+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.C4Y_PREG
[2023-06-30T07:39:01.904+0000] {SparkOracle2FileStore.py:80} INFO - processing 10 out of 48
[2023-06-30T07:39:01.904+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IEVS_PRISON_MATCH
[2023-06-30T07:39:01.904+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:39:03.121+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IEVS_PRISON_MATCH
[2023-06-30T07:39:03.121+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:39:04.420+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:39:04.975+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:39:04.975+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:39:06.247+0000] {SparkOracle2FileStore.py:103} INFO - 432367
[2023-06-30T07:39:06.247+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:39:07.541+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IEVS_PRISON_MATCH exists...
[2023-06-30T07:39:07.541+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:39:07.545+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:39:07.545+0000] {client.py:320} INFO - Fetching status for '/dev/IEVS_PRISON_MATCH'.
[2023-06-30T07:39:07.548+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:39:07.548+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:39:07.611+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:39:34.390+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IEVS_PRISON_MATCH
[2023-06-30T07:39:58.821+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IEVS_PRISON_MATCH
[2023-06-30T07:40:53.749+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IEVS_PRISON_MATCH
[2023-06-30T07:40:53.751+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IEVS_PRISON_MATCH
[2023-06-30T07:40:53.752+0000] {SparkOracle2FileStore.py:80} INFO - processing 11 out of 48
[2023-06-30T07:40:53.752+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.ORG_ACCT
[2023-06-30T07:40:53.752+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:40:54.808+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.ORG_ACCT
[2023-06-30T07:40:54.808+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:40:55.851+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:40:56.371+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:40:56.371+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:40:57.429+0000] {SparkOracle2FileStore.py:103} INFO - 30003
[2023-06-30T07:40:57.429+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:40:58.476+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/ORG_ACCT exists...
[2023-06-30T07:40:58.476+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:40:58.480+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:40:58.480+0000] {client.py:320} INFO - Fetching status for '/dev/ORG_ACCT'.
[2023-06-30T07:40:58.482+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:40:58.482+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:40:58.542+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:41:00.777+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table ORG_ACCT
[2023-06-30T07:41:03.133+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table ORG_ACCT
[2023-06-30T07:41:08.305+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table ORG_ACCT
[2023-06-30T07:41:08.305+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.ORG_ACCT
[2023-06-30T07:41:08.306+0000] {SparkOracle2FileStore.py:80} INFO - processing 12 out of 48
[2023-06-30T07:41:08.306+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.TRAIN_PGM
[2023-06-30T07:41:08.306+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:41:09.358+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.TRAIN_PGM
[2023-06-30T07:41:09.358+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:41:10.408+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:41:10.924+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:41:10.924+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:41:11.999+0000] {SparkOracle2FileStore.py:103} INFO - 18600
[2023-06-30T07:41:11.999+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:41:13.092+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/TRAIN_PGM exists...
[2023-06-30T07:41:13.092+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:41:13.096+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:41:13.096+0000] {client.py:320} INFO - Fetching status for '/dev/TRAIN_PGM'.
[2023-06-30T07:41:13.099+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:41:13.099+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:41:13.186+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:41:14.681+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table TRAIN_PGM
[2023-06-30T07:41:15.994+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table TRAIN_PGM
[2023-06-30T07:41:19.914+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table TRAIN_PGM
[2023-06-30T07:41:19.914+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.TRAIN_PGM
[2023-06-30T07:41:19.914+0000] {SparkOracle2FileStore.py:80} INFO - processing 13 out of 48
[2023-06-30T07:41:19.915+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IFD_SSI
[2023-06-30T07:41:19.915+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:41:20.976+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IFD_SSI
[2023-06-30T07:41:20.976+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:41:22.012+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:41:22.538+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:41:22.538+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:41:23.945+0000] {SparkOracle2FileStore.py:103} INFO - 775890
[2023-06-30T07:41:23.945+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:41:25.254+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IFD_SSI exists...
[2023-06-30T07:41:25.254+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:41:25.258+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:41:25.258+0000] {client.py:320} INFO - Fetching status for '/dev/IFD_SSI'.
[2023-06-30T07:41:25.261+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:41:25.261+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:41:25.305+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:41:58.416+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IFD_SSI
[2023-06-30T07:42:31.834+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IFD_SSI
[2023-06-30T07:43:46.174+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IFD_SSI
[2023-06-30T07:43:46.174+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IFD_SSI
[2023-06-30T07:43:46.174+0000] {SparkOracle2FileStore.py:80} INFO - processing 14 out of 48
[2023-06-30T07:43:46.175+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.C4Y_PERS_NON_COMPL
[2023-06-30T07:43:46.175+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:43:47.258+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.C4Y_PERS_NON_COMPL
[2023-06-30T07:43:47.258+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:43:48.296+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:43:48.785+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:43:48.785+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:43:50.440+0000] {SparkOracle2FileStore.py:103} INFO - 2477286
[2023-06-30T07:43:50.440+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:43:52.096+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/C4Y_PERS_NON_COMPL exists...
[2023-06-30T07:43:52.096+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:43:52.100+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:43:52.100+0000] {client.py:320} INFO - Fetching status for '/dev/C4Y_PERS_NON_COMPL'.
[2023-06-30T07:43:52.103+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:43:52.103+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:43:52.152+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:44:47.453+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table C4Y_PERS_NON_COMPL
[2023-06-30T07:45:43.498+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table C4Y_PERS_NON_COMPL
[2023-06-30T07:47:46.921+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table C4Y_PERS_NON_COMPL
[2023-06-30T07:47:46.922+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.C4Y_PERS_NON_COMPL
[2023-06-30T07:47:46.922+0000] {SparkOracle2FileStore.py:80} INFO - processing 15 out of 48
[2023-06-30T07:47:46.922+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.POS
[2023-06-30T07:47:46.922+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:47:47.930+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.POS
[2023-06-30T07:47:47.930+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:47:48.939+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:47:49.468+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:47:49.469+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:47:50.563+0000] {SparkOracle2FileStore.py:103} INFO - 346615
[2023-06-30T07:47:50.563+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:47:51.715+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/POS exists...
[2023-06-30T07:47:51.715+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:47:51.720+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:47:51.720+0000] {client.py:320} INFO - Fetching status for '/dev/POS'.
[2023-06-30T07:47:51.722+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:47:51.722+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:47:51.771+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:48:01.647+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table POS
[2023-06-30T07:48:12.265+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table POS
[2023-06-30T07:48:38.011+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table POS
[2023-06-30T07:48:38.011+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.POS
[2023-06-30T07:48:38.012+0000] {SparkOracle2FileStore.py:80} INFO - processing 16 out of 48
[2023-06-30T07:48:38.012+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.ADH_SUMM
[2023-06-30T07:48:38.012+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:48:39.112+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.ADH_SUMM
[2023-06-30T07:48:39.112+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:48:40.184+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:48:40.728+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:48:40.728+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:48:41.801+0000] {SparkOracle2FileStore.py:103} INFO - 16589
[2023-06-30T07:48:41.801+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:48:42.927+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/ADH_SUMM exists...
[2023-06-30T07:48:42.927+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:48:42.931+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:48:42.931+0000] {client.py:320} INFO - Fetching status for '/dev/ADH_SUMM'.
[2023-06-30T07:48:42.934+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:48:42.934+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:48:43.051+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:48:44.924+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table ADH_SUMM
[2023-06-30T07:48:46.452+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table ADH_SUMM
[2023-06-30T07:48:51.528+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table ADH_SUMM
[2023-06-30T07:48:51.529+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.ADH_SUMM
[2023-06-30T07:48:51.529+0000] {SparkOracle2FileStore.py:80} INFO - processing 17 out of 48
[2023-06-30T07:48:51.530+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_PGM_PARTICPTN
[2023-06-30T07:48:51.530+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:48:52.644+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_PGM_PARTICPTN
[2023-06-30T07:48:52.644+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:48:53.775+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:48:54.284+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:48:54.284+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:48:55.487+0000] {SparkOracle2FileStore.py:103} INFO - 227042
[2023-06-30T07:48:55.487+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:48:56.642+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WDTIP_PGM_PARTICPTN exists...
[2023-06-30T07:48:56.642+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:48:56.646+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:48:56.646+0000] {client.py:320} INFO - Fetching status for '/dev/WDTIP_PGM_PARTICPTN'.
[2023-06-30T07:48:56.648+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:48:56.648+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:48:56.745+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:49:05.409+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WDTIP_PGM_PARTICPTN
[2023-06-30T07:49:13.617+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WDTIP_PGM_PARTICPTN
[2023-06-30T07:49:33.167+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WDTIP_PGM_PARTICPTN
[2023-06-30T07:49:33.167+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WDTIP_PGM_PARTICPTN
[2023-06-30T07:49:33.168+0000] {SparkOracle2FileStore.py:80} INFO - processing 18 out of 48
[2023-06-30T07:49:33.168+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.HEAR_SUMM
[2023-06-30T07:49:33.168+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:49:34.205+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.HEAR_SUMM
[2023-06-30T07:49:34.205+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:49:35.359+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:49:35.884+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:49:35.884+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:49:37.458+0000] {SparkOracle2FileStore.py:103} INFO - 903275
[2023-06-30T07:49:37.458+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:49:39.134+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/HEAR_SUMM exists...
[2023-06-30T07:49:39.134+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:49:39.138+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:49:39.138+0000] {client.py:320} INFO - Fetching status for '/dev/HEAR_SUMM'.
[2023-06-30T07:49:39.140+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:49:39.141+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:49:39.233+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:50:03.973+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table HEAR_SUMM
[2023-06-30T07:50:28.431+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table HEAR_SUMM
[2023-06-30T07:51:30.286+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table HEAR_SUMM
[2023-06-30T07:51:30.287+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.HEAR_SUMM
[2023-06-30T07:51:30.287+0000] {SparkOracle2FileStore.py:80} INFO - processing 19 out of 48
[2023-06-30T07:51:30.287+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_ALERT
[2023-06-30T07:51:30.287+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:51:31.364+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_ALERT
[2023-06-30T07:51:31.364+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:51:32.439+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:51:32.952+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:51:32.952+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:51:34.376+0000] {SparkOracle2FileStore.py:103} INFO - 713240
[2023-06-30T07:51:34.376+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:51:35.818+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WDTIP_ALERT exists...
[2023-06-30T07:51:35.818+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:51:35.822+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:51:35.822+0000] {client.py:320} INFO - Fetching status for '/dev/WDTIP_ALERT'.
[2023-06-30T07:51:35.824+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:51:35.824+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:51:35.883+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:52:05.577+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WDTIP_ALERT
[2023-06-30T07:52:35.516+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WDTIP_ALERT
[2023-06-30T07:53:46.103+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WDTIP_ALERT
[2023-06-30T07:53:46.103+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WDTIP_ALERT
[2023-06-30T07:53:46.104+0000] {SparkOracle2FileStore.py:80} INFO - processing 20 out of 48
[2023-06-30T07:53:46.104+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IHSS_REFRL
[2023-06-30T07:53:46.104+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:53:47.166+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IHSS_REFRL
[2023-06-30T07:53:47.166+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:53:48.263+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:53:48.783+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:53:48.783+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:53:50.213+0000] {SparkOracle2FileStore.py:103} INFO - 994689
[2023-06-30T07:53:50.214+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:53:51.542+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IHSS_REFRL exists...
[2023-06-30T07:53:51.542+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:53:51.547+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:53:51.547+0000] {client.py:320} INFO - Fetching status for '/dev/IHSS_REFRL'.
[2023-06-30T07:53:51.549+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:53:51.549+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:53:51.593+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:54:20.676+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IHSS_REFRL
[2023-06-30T07:54:50.024+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IHSS_REFRL
[2023-06-30T07:55:54.260+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IHSS_REFRL
[2023-06-30T07:55:54.260+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IHSS_REFRL
[2023-06-30T07:55:54.261+0000] {SparkOracle2FileStore.py:80} INFO - processing 21 out of 48
[2023-06-30T07:55:54.261+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.SFIS_COUNTY_INFO
[2023-06-30T07:55:54.261+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:55:55.292+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.SFIS_COUNTY_INFO
[2023-06-30T07:55:55.292+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:55:56.344+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:55:56.874+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:55:56.874+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:55:58.082+0000] {SparkOracle2FileStore.py:103} INFO - 375901
[2023-06-30T07:55:58.083+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:55:59.273+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/SFIS_COUNTY_INFO exists...
[2023-06-30T07:55:59.273+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:55:59.277+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:55:59.277+0000] {client.py:320} INFO - Fetching status for '/dev/SFIS_COUNTY_INFO'.
[2023-06-30T07:55:59.280+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:55:59.280+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:55:59.322+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:56:11.645+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table SFIS_COUNTY_INFO
[2023-06-30T07:56:23.928+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table SFIS_COUNTY_INFO
[2023-06-30T07:56:51.786+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table SFIS_COUNTY_INFO
[2023-06-30T07:56:51.786+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.SFIS_COUNTY_INFO
[2023-06-30T07:56:51.787+0000] {SparkOracle2FileStore.py:80} INFO - processing 22 out of 48
[2023-06-30T07:56:51.787+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.STAFF
[2023-06-30T07:56:51.787+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:56:52.864+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.STAFF
[2023-06-30T07:56:52.864+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:56:53.956+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:56:54.524+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:56:54.524+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:56:55.753+0000] {SparkOracle2FileStore.py:103} INFO - 345367
[2023-06-30T07:56:55.753+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:56:56.890+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/STAFF exists...
[2023-06-30T07:56:56.891+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:56:56.895+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:56:56.895+0000] {client.py:320} INFO - Fetching status for '/dev/STAFF'.
[2023-06-30T07:56:56.897+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:56:56.897+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:56:56.949+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:57:09.891+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table STAFF
[2023-06-30T07:57:22.795+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table STAFF
[2023-06-30T07:57:52.900+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table STAFF
[2023-06-30T07:57:52.901+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.STAFF
[2023-06-30T07:57:52.901+0000] {SparkOracle2FileStore.py:80} INFO - processing 23 out of 48
[2023-06-30T07:57:52.901+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.MEDS_ALERT_CONFIG
[2023-06-30T07:57:52.901+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:57:54.033+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.MEDS_ALERT_CONFIG
[2023-06-30T07:57:54.033+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:57:55.134+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:57:55.686+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:57:55.686+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:57:56.800+0000] {SparkOracle2FileStore.py:103} INFO - 44544
[2023-06-30T07:57:56.800+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:57:57.944+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/MEDS_ALERT_CONFIG exists...
[2023-06-30T07:57:57.944+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:57:57.948+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:57:57.948+0000] {client.py:320} INFO - Fetching status for '/dev/MEDS_ALERT_CONFIG'.
[2023-06-30T07:57:57.950+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:57:57.950+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:57:57.997+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:58:00.259+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table MEDS_ALERT_CONFIG
[2023-06-30T07:58:02.229+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table MEDS_ALERT_CONFIG
[2023-06-30T07:58:08.479+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table MEDS_ALERT_CONFIG
[2023-06-30T07:58:08.479+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.MEDS_ALERT_CONFIG
[2023-06-30T07:58:08.479+0000] {SparkOracle2FileStore.py:80} INFO - processing 24 out of 48
[2023-06-30T07:58:08.479+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.BATCH_JOB
[2023-06-30T07:58:08.479+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:58:09.600+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.BATCH_JOB
[2023-06-30T07:58:09.600+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:58:10.842+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:58:11.380+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:58:11.381+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:58:12.448+0000] {SparkOracle2FileStore.py:103} INFO - 35062
[2023-06-30T07:58:12.448+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:58:13.545+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/BATCH_JOB exists...
[2023-06-30T07:58:13.546+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:58:13.550+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:58:13.550+0000] {client.py:320} INFO - Fetching status for '/dev/BATCH_JOB'.
[2023-06-30T07:58:13.553+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:58:13.553+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:58:13.592+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:58:15.420+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table BATCH_JOB
[2023-06-30T07:58:17.461+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table BATCH_JOB
[2023-06-30T07:58:21.683+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table BATCH_JOB
[2023-06-30T07:58:21.683+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.BATCH_JOB
[2023-06-30T07:58:21.684+0000] {SparkOracle2FileStore.py:80} INFO - processing 25 out of 48
[2023-06-30T07:58:21.684+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IHSS_CASE
[2023-06-30T07:58:21.684+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:58:22.722+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IHSS_CASE
[2023-06-30T07:58:22.722+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:58:23.736+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:58:24.220+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:58:24.220+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:58:25.379+0000] {SparkOracle2FileStore.py:103} INFO - 864326
[2023-06-30T07:58:25.379+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:58:26.552+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IHSS_CASE exists...
[2023-06-30T07:58:26.552+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:58:26.556+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:58:26.556+0000] {client.py:320} INFO - Fetching status for '/dev/IHSS_CASE'.
[2023-06-30T07:58:26.559+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:58:26.559+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:58:26.608+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:58:41.286+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IHSS_CASE
[2023-06-30T07:58:55.584+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IHSS_CASE
[2023-06-30T07:59:30.477+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IHSS_CASE
[2023-06-30T07:59:30.477+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IHSS_CASE
[2023-06-30T07:59:30.478+0000] {SparkOracle2FileStore.py:80} INFO - processing 26 out of 48
[2023-06-30T07:59:30.478+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column HST_ID on table PR_LRS.RULES_ADMIN_HST
[2023-06-30T07:59:30.478+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:59:31.562+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column HST_ID on table PR_LRS.RULES_ADMIN_HST
[2023-06-30T07:59:31.562+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:59:32.624+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:59:33.125+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:59:33.125+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:59:34.242+0000] {SparkOracle2FileStore.py:103} INFO - 54080
[2023-06-30T07:59:34.242+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:59:35.314+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/RULES_ADMIN_HST exists...
[2023-06-30T07:59:35.315+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:59:35.319+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:59:35.319+0000] {client.py:320} INFO - Fetching status for '/dev/RULES_ADMIN_HST'.
[2023-06-30T07:59:35.322+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:59:35.322+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:59:35.388+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:59:38.765+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table RULES_ADMIN_HST
[2023-06-30T07:59:41.081+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table RULES_ADMIN_HST
[2023-06-30T07:59:47.821+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table RULES_ADMIN_HST
[2023-06-30T07:59:47.821+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.RULES_ADMIN_HST
[2023-06-30T07:59:47.821+0000] {SparkOracle2FileStore.py:80} INFO - processing 27 out of 48
[2023-06-30T07:59:47.821+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.TAX_INTRCPT
[2023-06-30T07:59:47.821+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:59:48.859+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.TAX_INTRCPT
[2023-06-30T07:59:48.859+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T07:59:49.836+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T07:59:50.360+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T07:59:50.361+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:59:51.456+0000] {SparkOracle2FileStore.py:103} INFO - 173371
[2023-06-30T07:59:51.456+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T07:59:52.607+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/TAX_INTRCPT exists...
[2023-06-30T07:59:52.607+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T07:59:52.612+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T07:59:52.612+0000] {client.py:320} INFO - Fetching status for '/dev/TAX_INTRCPT'.
[2023-06-30T07:59:52.614+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T07:59:52.614+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T07:59:52.659+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T07:59:58.468+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table TAX_INTRCPT
[2023-06-30T08:00:04.086+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table TAX_INTRCPT
[2023-06-30T08:00:17.604+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table TAX_INTRCPT
[2023-06-30T08:00:17.604+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.TAX_INTRCPT
[2023-06-30T08:00:17.605+0000] {SparkOracle2FileStore.py:80} INFO - processing 28 out of 48
[2023-06-30T08:00:17.605+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column HST_ID on table PR_LRS.WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:00:17.605+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:00:18.664+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column HST_ID on table PR_LRS.WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:00:18.664+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:00:19.732+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:00:20.277+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:00:20.277+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:00:21.456+0000] {SparkOracle2FileStore.py:103} INFO - 347499
[2023-06-30T08:00:21.456+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:00:22.630+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WTW_24_MONTH_SIGN_DATE_HST exists...
[2023-06-30T08:00:22.630+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:00:22.635+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:00:22.635+0000] {client.py:320} INFO - Fetching status for '/dev/WTW_24_MONTH_SIGN_DATE_HST'.
[2023-06-30T08:00:22.637+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:00:22.637+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:00:22.684+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:00:36.817+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:00:50.688+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:01:21.929+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:01:21.929+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WTW_24_MONTH_SIGN_DATE_HST
[2023-06-30T08:01:21.930+0000] {SparkOracle2FileStore.py:80} INFO - processing 29 out of 48
[2023-06-30T08:01:21.930+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:21.930+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:01:22.945+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:22.945+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:01:24.023+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:01:24.599+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:01:24.599+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:01:25.658+0000] {SparkOracle2FileStore.py:103} INFO - 18240
[2023-06-30T08:01:25.658+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:01:26.764+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/COUNTY_PARAMTR_ADMIN exists...
[2023-06-30T08:01:26.764+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:01:26.768+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:01:26.768+0000] {client.py:320} INFO - Fetching status for '/dev/COUNTY_PARAMTR_ADMIN'.
[2023-06-30T08:01:26.770+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:01:26.770+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:01:26.814+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:01:28.348+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:29.423+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:33.380+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:33.380+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.COUNTY_PARAMTR_ADMIN
[2023-06-30T08:01:33.381+0000] {SparkOracle2FileStore.py:80} INFO - processing 30 out of 48
[2023-06-30T08:01:33.381+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.OCAT
[2023-06-30T08:01:33.381+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:01:34.490+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.OCAT
[2023-06-30T08:01:34.491+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:01:35.533+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:01:36.030+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:01:36.031+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:01:38.194+0000] {SparkOracle2FileStore.py:103} INFO - 176399
[2023-06-30T08:01:38.194+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:01:39.347+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/OCAT exists...
[2023-06-30T08:01:39.348+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:01:39.351+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:01:39.352+0000] {client.py:320} INFO - Fetching status for '/dev/OCAT'.
[2023-06-30T08:01:39.354+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:01:39.354+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:01:39.394+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:01:50.853+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table OCAT
[2023-06-30T08:02:02.065+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table OCAT
[2023-06-30T08:02:37.275+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table OCAT
[2023-06-30T08:02:37.275+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.OCAT
[2023-06-30T08:02:37.276+0000] {SparkOracle2FileStore.py:80} INFO - processing 31 out of 48
[2023-06-30T08:02:37.276+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:37.276+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:02:38.359+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:38.359+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:02:39.414+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:02:39.970+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:02:39.970+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:02:41.072+0000] {SparkOracle2FileStore.py:103} INFO - 17802
[2023-06-30T08:02:41.072+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:02:42.149+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WDTIP_EXCEED_CLOCK exists...
[2023-06-30T08:02:42.149+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:02:42.152+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:02:42.153+0000] {client.py:320} INFO - Fetching status for '/dev/WDTIP_EXCEED_CLOCK'.
[2023-06-30T08:02:42.155+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:02:42.155+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:02:42.219+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:02:43.928+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:45.179+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:49.674+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:49.674+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WDTIP_EXCEED_CLOCK
[2023-06-30T08:02:49.675+0000] {SparkOracle2FileStore.py:80} INFO - processing 32 out of 48
[2023-06-30T08:02:49.675+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.AUTO_ACTN
[2023-06-30T08:02:49.675+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:02:50.744+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.AUTO_ACTN
[2023-06-30T08:02:50.744+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:02:51.816+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:02:52.331+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:02:52.331+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:02:53.349+0000] {SparkOracle2FileStore.py:103} INFO - 13113
[2023-06-30T08:02:53.349+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:02:54.413+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/AUTO_ACTN exists...
[2023-06-30T08:02:54.413+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:02:54.417+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:02:54.417+0000] {client.py:320} INFO - Fetching status for '/dev/AUTO_ACTN'.
[2023-06-30T08:02:54.420+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:02:54.420+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:02:54.485+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:02:55.892+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table AUTO_ACTN
[2023-06-30T08:02:56.872+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table AUTO_ACTN
[2023-06-30T08:03:00.561+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table AUTO_ACTN
[2023-06-30T08:03:00.561+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.AUTO_ACTN
[2023-06-30T08:03:00.561+0000] {SparkOracle2FileStore.py:80} INFO - processing 33 out of 48
[2023-06-30T08:03:00.562+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IFD_DUPL_DETL
[2023-06-30T08:03:00.562+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:03:01.568+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IFD_DUPL_DETL
[2023-06-30T08:03:01.569+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:03:02.617+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:03:03.173+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:03:03.173+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:03:04.380+0000] {SparkOracle2FileStore.py:103} INFO - 353610
[2023-06-30T08:03:04.380+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:03:05.563+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IFD_DUPL_DETL exists...
[2023-06-30T08:03:05.563+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:03:05.567+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:03:05.567+0000] {client.py:320} INFO - Fetching status for '/dev/IFD_DUPL_DETL'.
[2023-06-30T08:03:05.570+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:03:05.570+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:03:05.610+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:03:19.181+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IFD_DUPL_DETL
[2023-06-30T08:03:32.332+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IFD_DUPL_DETL
[2023-06-30T08:04:02.870+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IFD_DUPL_DETL
[2023-06-30T08:04:02.870+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IFD_DUPL_DETL
[2023-06-30T08:04:02.870+0000] {SparkOracle2FileStore.py:80} INFO - processing 34 out of 48
[2023-06-30T08:04:02.871+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-06-30T08:04:02.871+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:04:03.902+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.EBT_FRAUD_ACTIV
[2023-06-30T08:04:03.902+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:04:04.985+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:04:05.541+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:04:05.542+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:04:06.656+0000] {SparkOracle2FileStore.py:103} INFO - 21414
[2023-06-30T08:04:06.656+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:04:07.754+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/EBT_FRAUD_ACTIV exists...
[2023-06-30T08:04:07.754+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:04:07.758+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:04:07.758+0000] {client.py:320} INFO - Fetching status for '/dev/EBT_FRAUD_ACTIV'.
[2023-06-30T08:04:07.760+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:04:07.760+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:04:07.802+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:04:09.583+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table EBT_FRAUD_ACTIV
[2023-06-30T08:04:11.553+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table EBT_FRAUD_ACTIV
[2023-06-30T08:04:15.564+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table EBT_FRAUD_ACTIV
[2023-06-30T08:04:15.565+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.EBT_FRAUD_ACTIV
[2023-06-30T08:04:15.565+0000] {SparkOracle2FileStore.py:80} INFO - processing 35 out of 48
[2023-06-30T08:04:15.565+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.GA_GR_TIME_LIMIT
[2023-06-30T08:04:15.565+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:04:16.635+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.GA_GR_TIME_LIMIT
[2023-06-30T08:04:16.635+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:04:17.657+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:04:18.230+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:04:18.231+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:04:19.729+0000] {SparkOracle2FileStore.py:103} INFO - 556545
[2023-06-30T08:04:19.729+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:04:21.690+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/GA_GR_TIME_LIMIT exists...
[2023-06-30T08:04:21.690+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:04:21.694+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:04:21.694+0000] {client.py:320} INFO - Fetching status for '/dev/GA_GR_TIME_LIMIT'.
[2023-06-30T08:04:21.696+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:04:21.696+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:04:21.743+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:04:35.706+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table GA_GR_TIME_LIMIT
[2023-06-30T08:04:51.907+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table GA_GR_TIME_LIMIT
[2023-06-30T08:05:16.341+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table GA_GR_TIME_LIMIT
[2023-06-30T08:05:16.341+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.GA_GR_TIME_LIMIT
[2023-06-30T08:05:16.342+0000] {SparkOracle2FileStore.py:80} INFO - processing 36 out of 48
[2023-06-30T08:05:16.342+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.GRP_HOMES
[2023-06-30T08:05:16.342+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:05:17.440+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.GRP_HOMES
[2023-06-30T08:05:17.440+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:05:18.483+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:05:19.043+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:05:19.043+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:05:20.215+0000] {SparkOracle2FileStore.py:103} INFO - 35184
[2023-06-30T08:05:20.215+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:05:21.302+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/GRP_HOMES exists...
[2023-06-30T08:05:21.302+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:05:21.306+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:05:21.306+0000] {client.py:320} INFO - Fetching status for '/dev/GRP_HOMES'.
[2023-06-30T08:05:21.309+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:05:21.309+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:05:21.370+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:05:25.932+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table GRP_HOMES
[2023-06-30T08:05:30.073+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table GRP_HOMES
[2023-06-30T08:05:42.460+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table GRP_HOMES
[2023-06-30T08:05:42.460+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.GRP_HOMES
[2023-06-30T08:05:42.460+0000] {SparkOracle2FileStore.py:80} INFO - processing 37 out of 48
[2023-06-30T08:05:42.460+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.ASSET_VERIF
[2023-06-30T08:05:42.460+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:05:43.539+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.ASSET_VERIF
[2023-06-30T08:05:43.539+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:05:44.639+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:05:45.173+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:05:45.173+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:05:46.787+0000] {SparkOracle2FileStore.py:103} INFO - 1628991
[2023-06-30T08:05:46.787+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:05:48.336+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/ASSET_VERIF exists...
[2023-06-30T08:05:48.336+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:05:48.340+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:05:48.340+0000] {client.py:320} INFO - Fetching status for '/dev/ASSET_VERIF'.
[2023-06-30T08:05:48.342+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:05:48.342+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:05:48.404+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:06:35.829+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table ASSET_VERIF
[2023-06-30T08:07:23.812+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table ASSET_VERIF
[2023-06-30T08:09:05.085+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table ASSET_VERIF
[2023-06-30T08:09:05.085+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.ASSET_VERIF
[2023-06-30T08:09:05.092+0000] {SparkOracle2FileStore.py:80} INFO - processing 38 out of 48
[2023-06-30T08:09:05.092+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.FUND_CODE_MAP
[2023-06-30T08:09:05.092+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:06.201+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.FUND_CODE_MAP
[2023-06-30T08:09:06.202+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:07.261+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:09:07.765+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:09:07.765+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:08.843+0000] {SparkOracle2FileStore.py:103} INFO - 31115
[2023-06-30T08:09:08.843+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:09.972+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/FUND_CODE_MAP exists...
[2023-06-30T08:09:09.972+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:09:09.997+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:09:09.997+0000] {client.py:320} INFO - Fetching status for '/dev/FUND_CODE_MAP'.
[2023-06-30T08:09:09.999+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:09:10.000+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:09:10.064+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:09:12.477+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table FUND_CODE_MAP
[2023-06-30T08:09:15.042+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table FUND_CODE_MAP
[2023-06-30T08:09:22.323+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table FUND_CODE_MAP
[2023-06-30T08:09:22.323+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.FUND_CODE_MAP
[2023-06-30T08:09:22.324+0000] {SparkOracle2FileStore.py:80} INFO - processing 39 out of 48
[2023-06-30T08:09:22.324+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:22.324+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:23.417+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:23.417+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:24.448+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:09:24.971+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:09:24.971+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:26.009+0000] {SparkOracle2FileStore.py:103} INFO - 90422
[2023-06-30T08:09:26.009+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:27.124+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/C4Y_OTHER_PGM_ASSIST exists...
[2023-06-30T08:09:27.124+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:09:27.128+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:09:27.128+0000] {client.py:320} INFO - Fetching status for '/dev/C4Y_OTHER_PGM_ASSIST'.
[2023-06-30T08:09:27.130+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:09:27.130+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:09:27.198+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:09:30.630+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:33.704+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:41.800+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:41.801+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.C4Y_OTHER_PGM_ASSIST
[2023-06-30T08:09:41.801+0000] {SparkOracle2FileStore.py:80} INFO - processing 40 out of 48
[2023-06-30T08:09:41.802+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.IEVS_FF_MEDS
[2023-06-30T08:09:41.802+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:42.906+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.IEVS_FF_MEDS
[2023-06-30T08:09:42.906+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:09:43.953+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:09:44.480+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:09:44.480+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:45.690+0000] {SparkOracle2FileStore.py:103} INFO - 123592
[2023-06-30T08:09:45.690+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:09:46.794+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/IEVS_FF_MEDS exists...
[2023-06-30T08:09:46.794+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:09:46.799+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:09:46.799+0000] {client.py:320} INFO - Fetching status for '/dev/IEVS_FF_MEDS'.
[2023-06-30T08:09:46.802+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:09:46.802+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:09:46.844+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:09:54.947+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table IEVS_FF_MEDS
[2023-06-30T08:10:03.994+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table IEVS_FF_MEDS
[2023-06-30T08:10:26.149+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table IEVS_FF_MEDS
[2023-06-30T08:10:26.150+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.IEVS_FF_MEDS
[2023-06-30T08:10:26.150+0000] {SparkOracle2FileStore.py:80} INFO - processing 41 out of 48
[2023-06-30T08:10:26.150+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.COLLAB_CONTRACT
[2023-06-30T08:10:26.150+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:27.224+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.COLLAB_CONTRACT
[2023-06-30T08:10:27.224+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:28.337+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:10:28.812+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:10:28.812+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:29.853+0000] {SparkOracle2FileStore.py:103} INFO - 53776
[2023-06-30T08:10:29.853+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:30.932+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/COLLAB_CONTRACT exists...
[2023-06-30T08:10:30.932+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:10:30.936+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:10:30.936+0000] {client.py:320} INFO - Fetching status for '/dev/COLLAB_CONTRACT'.
[2023-06-30T08:10:30.938+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:10:30.938+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:10:30.996+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:10:33.300+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table COLLAB_CONTRACT
[2023-06-30T08:10:35.155+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table COLLAB_CONTRACT
[2023-06-30T08:10:40.066+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table COLLAB_CONTRACT
[2023-06-30T08:10:40.066+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.COLLAB_CONTRACT
[2023-06-30T08:10:40.066+0000] {SparkOracle2FileStore.py:80} INFO - processing 42 out of 48
[2023-06-30T08:10:40.067+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:40.067+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:41.176+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:41.176+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:42.258+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:10:42.762+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:10:42.762+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:43.830+0000] {SparkOracle2FileStore.py:103} INFO - 23587
[2023-06-30T08:10:43.831+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:44.857+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/WDTIP_APPRCH_CLOCK exists...
[2023-06-30T08:10:44.857+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:10:44.861+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:10:44.861+0000] {client.py:320} INFO - Fetching status for '/dev/WDTIP_APPRCH_CLOCK'.
[2023-06-30T08:10:44.864+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:10:44.864+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:10:44.918+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:10:46.751+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:48.363+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:53.005+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:53.005+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.WDTIP_APPRCH_CLOCK
[2023-06-30T08:10:53.006+0000] {SparkOracle2FileStore.py:80} INFO - processing 43 out of 48
[2023-06-30T08:10:53.006+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:10:53.006+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:54.048+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:10:54.048+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:10:55.147+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:10:55.700+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:10:55.700+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:56.784+0000] {SparkOracle2FileStore.py:103} INFO - 54196
[2023-06-30T08:10:56.784+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:10:57.827+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/EXTRNL_STAFF_COUNTY_STAT exists...
[2023-06-30T08:10:57.827+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:10:57.831+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:10:57.831+0000] {client.py:320} INFO - Fetching status for '/dev/EXTRNL_STAFF_COUNTY_STAT'.
[2023-06-30T08:10:57.834+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:10:57.834+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:10:57.903+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:11:00.388+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:11:02.762+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:11:09.149+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:11:09.149+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.EXTRNL_STAFF_COUNTY_STAT
[2023-06-30T08:11:09.149+0000] {SparkOracle2FileStore.py:80} INFO - processing 44 out of 48
[2023-06-30T08:11:09.150+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.VEND_IDENTIF
[2023-06-30T08:11:09.150+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:11:10.239+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.VEND_IDENTIF
[2023-06-30T08:11:10.239+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:11:11.294+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:11:11.837+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:11:11.837+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:11:13.075+0000] {SparkOracle2FileStore.py:103} INFO - 889753
[2023-06-30T08:11:13.075+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:11:14.316+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/VEND_IDENTIF exists...
[2023-06-30T08:11:14.316+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:11:14.322+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:11:14.322+0000] {client.py:320} INFO - Fetching status for '/dev/VEND_IDENTIF'.
[2023-06-30T08:11:14.324+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:11:14.324+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:11:14.367+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:11:38.338+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table VEND_IDENTIF
[2023-06-30T08:12:01.699+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table VEND_IDENTIF
[2023-06-30T08:12:53.534+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table VEND_IDENTIF
[2023-06-30T08:12:53.534+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.VEND_IDENTIF
[2023-06-30T08:12:53.536+0000] {SparkOracle2FileStore.py:80} INFO - processing 45 out of 48
[2023-06-30T08:12:53.536+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.OCAT_PERS
[2023-06-30T08:12:53.536+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:12:54.631+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column ID on table PR_LRS.OCAT_PERS
[2023-06-30T08:12:54.631+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:12:55.670+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:12:56.157+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:12:56.157+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:12:57.417+0000] {SparkOracle2FileStore.py:103} INFO - 464630
[2023-06-30T08:12:57.417+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:12:58.699+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/OCAT_PERS exists...
[2023-06-30T08:12:58.699+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:12:58.705+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:12:58.705+0000] {client.py:320} INFO - Fetching status for '/dev/OCAT_PERS'.
[2023-06-30T08:12:58.707+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:12:58.707+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:12:58.759+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:13:25.949+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table OCAT_PERS
[2023-06-30T08:13:50.602+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table OCAT_PERS
[2023-06-30T08:14:50.191+0000] {SparkOracle2FileStore.py:157} INFO - no rows to delete on table OCAT_PERS
[2023-06-30T08:14:50.191+0000] {SparkOracle2FileStore.py:165} INFO - Table Processing complete for PR_LRS.OCAT_PERS
[2023-06-30T08:14:50.201+0000] {SparkOracle2FileStore.py:80} INFO - processing 46 out of 48
[2023-06-30T08:14:50.201+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column HST_ID on table PR_LRS.MEDS_ALERT_CONFIG_HST
[2023-06-30T08:14:50.201+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:14:51.309+0000] {logging_mixin.py:149} INFO - Attempting to get max value by column HST_ID on table PR_LRS.MEDS_ALERT_CONFIG_HST
[2023-06-30T08:14:51.309+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
[2023-06-30T08:14:52.348+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table...
[2023-06-30T08:14:52.895+0000] {SparkOracle2FileStore.py:100} INFO - Filter Processor Disabled
[2023-06-30T08:14:52.895+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:14:54.108+0000] {SparkOracle2FileStore.py:103} INFO - 176919
[2023-06-30T08:14:54.108+0000] {logging_mixin.py:149} INFO - Attempting to read oracle rowcount by query...
[2023-06-30T08:14:55.264+0000] {logging_mixin.py:149} INFO - Checking if HDFS Directory /dev/MEDS_ALERT_CONFIG_HST exists...
[2023-06-30T08:14:55.264+0000] {client.py:1123} INFO - Listing '/'.
[2023-06-30T08:14:55.313+0000] {logging_mixin.py:149} INFO - HDFS Connection Successfull
[2023-06-30T08:14:55.313+0000] {client.py:320} INFO - Fetching status for '/dev/MEDS_ALERT_CONFIG_HST'.
[2023-06-30T08:14:55.316+0000] {logging_mixin.py:149} INFO - HDFS Path Exists
[2023-06-30T08:14:55.316+0000] {SparkOracle2FileStore.py:113} INFO - Should go in here if table dir exists
[2023-06-30T08:14:55.409+0000] {SparkOracle2FileStore.py:126} INFO - read dir table should not fail and go in here...
[2023-06-30T08:15:04.418+0000] {SparkOracle2FileStore.py:139} INFO - no rows to upsert on table MEDS_ALERT_CONFIG_HST
[2023-06-30T08:17:54.805+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 212, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 243, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 811, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:18:38.110+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 212, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 243, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 811, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:18:58.786+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 212, in heartbeat
    heartbeat_callback(session)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 73, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/local_task_job_runner.py", line 243, in heartbeat_callback
    self.task_instance.refresh_from_db()
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/utils/session.py", line 76, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/models/taskinstance.py", line 811, in refresh_from_db
    ti = qry.one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2850, in one_or_none
    return self._iter().one_or_none()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/query.py", line 2916, in _iter
    result = self.session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:03.201+0000] {SparkOracle2FileStore.py:143} INFO - Updating rows on table MEDS_ALERT_CONFIG_HST
[2023-06-30T08:19:04.639+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:10.817+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:21.240+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:32.841+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:37.851+0000] {job.py:216} ERROR - Job heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not translate host name "postgres" to address: Temporary failure in name resolution


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.10/site-packages/airflow/jobs/job.py", line 187, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3056, in merge
    return self._merge(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 3136, in _merge
    merged = self.get(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2853, in get
    return self._get_impl(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 2975, in _get_impl
    return db_load_fn(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/loading.py", line 530, in load_on_pk_identity
    session.execute(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1716, in execute
    conn = self._connection_for_bind(bind)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 1555, in _connection_for_bind
    return self._transaction._connection_for_bind(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/orm/session.py", line 750, in _connection_for_bind
    conn = bind.connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3320, in connect
    return self._connection_cls(self, close_with_result=close_with_result)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 96, in __init__
    else engine.raw_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3399, in raw_connection
    return self._wrap_pool_connect(self.pool.connect, _connection)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3369, in _wrap_pool_connect
    Connection._handle_dbapi_exception_noconnection(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 2203, in _handle_dbapi_exception_noconnection
    util.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/base.py", line 3366, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 327, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 894, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 493, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/impl.py", line 256, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 273, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 388, in __init__
    self.__connect()
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 690, in __connect
    with util.safe_reraise():
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    compat.raise_(
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/util/compat.py", line 211, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/pool/base.py", line 686, in __connect
    self.dbapi_connection = connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/create.py", line 574, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/sqlalchemy/engine/default.py", line 598, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.10/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name "postgres" to address: Temporary failure in name resolution

(Background on this error at: https://sqlalche.me/e/14/e3q8)
[2023-06-30T08:19:40.189+0000] {SparkOracle2FileStore.py:176} INFO - Error Processing Table PR_LRS.MEDS_ALERT_CONFIG_HST:An error occurred while calling o3021.execute.
: java.lang.RuntimeException: java.net.UnknownHostException: namenode
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:470)
	at org.apache.spark.sql.delta.storage.HDFSLogStore.getFileContext(HDFSLogStore.scala:51)
	at org.apache.spark.sql.delta.storage.HDFSLogStore.writeInternal(HDFSLogStore.scala:86)
	at org.apache.spark.sql.delta.storage.HDFSLogStore.write(HDFSLogStore.scala:76)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.writeCommitFile(OptimisticTransaction.scala:1525)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.writeCommitFile$(OptimisticTransaction.scala:1517)
	at org.apache.spark.sql.delta.OptimisticTransaction.writeCommitFile(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommit(OptimisticTransaction.scala:1440)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommit$(OptimisticTransaction.scala:1411)
	at org.apache.spark.sql.delta.OptimisticTransaction.doCommit(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$doCommitRetryIteratively$3(OptimisticTransaction.scala:1380)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$doCommitRetryIteratively$2(OptimisticTransaction.scala:1377)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$doCommitRetryIteratively$1(OptimisticTransaction.scala:1377)
	at org.apache.spark.sql.delta.DeltaLog.lockInterruptibly(DeltaLog.scala:158)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.lockCommitIfEnabled(OptimisticTransaction.scala:1353)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommitRetryIteratively(OptimisticTransaction.scala:1371)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.doCommitRetryIteratively$(OptimisticTransaction.scala:1367)
	at org.apache.spark.sql.delta.OptimisticTransaction.doCommitRetryIteratively(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.liftedTree1$1(OptimisticTransaction.scala:961)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.$anonfun$commitImpl$1(OptimisticTransaction.scala:899)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
	at org.apache.spark.sql.delta.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.commitImpl(OptimisticTransaction.scala:896)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.commitImpl$(OptimisticTransaction.scala:892)
	at org.apache.spark.sql.delta.OptimisticTransaction.commitImpl(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.commitIfNeeded(OptimisticTransaction.scala:874)
	at org.apache.spark.sql.delta.OptimisticTransactionImpl.commitIfNeeded$(OptimisticTransaction.scala:873)
	at org.apache.spark.sql.delta.OptimisticTransaction.commitIfNeeded(OptimisticTransaction.scala:129)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:423)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:363)
	at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:233)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:363)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:140)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:138)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:234)
	at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:133)
	at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:128)
	at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:117)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:234)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:132)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:122)
	at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:112)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:234)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:361)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:356)
	at org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries(MergeIntoMaterializeSource.scala:102)
	at org.apache.spark.sql.delta.commands.merge.MergeIntoMaterializeSource.runWithMaterializedSourceLostRetries$(MergeIntoMaterializeSource.scala:90)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.runWithMaterializedSourceLostRetries(MergeIntoCommand.scala:234)
	at org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:356)
	at io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:290)
	at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:105)
	at org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:91)
	at io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:148)
	at io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:266)
	at jdk.internal.reflect.GeneratedMethodAccessor129.invoke(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.net.UnknownHostException: namenode
	at org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:466)
	at org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:374)
	at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:308)
	at org.apache.hadoop.fs.Hdfs.<init>(Hdfs.java:91)
	at jdk.internal.reflect.GeneratedConstructorAccessor160.newInstance(Unknown Source)
	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)
	at org.apache.hadoop.fs.AbstractFileSystem.newInstance(AbstractFileSystem.java:143)
	at org.apache.hadoop.fs.AbstractFileSystem.createFileSystem(AbstractFileSystem.java:181)
	at org.apache.hadoop.fs.AbstractFileSystem.get(AbstractFileSystem.java:266)
	at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:342)
	at org.apache.hadoop.fs.FileContext$2.run(FileContext.java:339)
	at java.base/java.security.AccessController.doPrivileged(Native Method)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:423)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1878)
	at org.apache.hadoop.fs.FileContext.getAbstractFileSystem(FileContext.java:339)
	at org.apache.hadoop.fs.FileContext.getFileContext(FileContext.java:465)
	... 87 more

[2023-06-30T08:19:40.190+0000] {SparkOracle2FileStore.py:80} INFO - processing 46 out of 48
[2023-06-30T08:19:40.190+0000] {logging_mixin.py:149} INFO - Attempting to get lowest value by column ID on table PR_LRS.WPR_SAMPLE_CASE
[2023-06-30T08:19:40.190+0000] {logging_mixin.py:149} INFO - Attempting to read oracle table by query...
